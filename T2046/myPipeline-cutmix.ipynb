{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ec0646-6fbc-453d-86b7-46bcbd89adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41b7a87f-1d69-4cd2-8937-c25ff178b497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPUtil\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Module\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from pytorch_pretrained_vit import ViT\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import imutils\n",
    "\n",
    "import random\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9ba46d9-cbdf-4bef-b0f3-289087b8f43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "cpu = torch.device('cpu')\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57beb65c-6bdd-476f-9fbf-b795966f4eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a921e550-b7d1-4ecd-8611-97f52333b5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/opt/ml/input/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991e893b-43e0-45b7-9c86-181c92bf7c30",
   "metadata": {},
   "source": [
    "## DATASET CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e14ef13a-136f-48a6-a222-097cbd229fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class maskDataset(Dataset):\n",
    "    def __init__(self, data_path, transform, n_val_per_class = 25, seed = 10, train = True):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "    \n",
    "        self.train_data_path = os.path.join(data_path, 'train')\n",
    "        self.train_images_path = os.path.join(self.train_data_path, 'images')\n",
    "        self.train_data_csv_path = os.path.join(self.train_data_path, 'train.csv')\n",
    "        \n",
    "        self.train_data_path_dataframe = pd.read_csv(self.train_data_csv_path)\n",
    "        \n",
    "        \n",
    "        self.AGE_MAP = {0:'age<30', 1:'30<=age<60', 2:'age>=60'}\n",
    "        self.CLASS_MAP = {'mask':0, 'incorrect_mask':6, 'normal':12,\n",
    "                          'male':0, 'female':3,\n",
    "                          0:0, 1:1, 2:2}\n",
    "        \n",
    "        self.CLASS_TO_ONEHOT_DICT = {}\n",
    "        self.CLASS_TO_LABEL_DICT = {}\n",
    "        \n",
    "        def gen_onehot_label(gender, age, filename):\n",
    "            rePattern = re.compile('[0-9]')\n",
    "            one_hot_vector = torch.tensor([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],dtype=torch.float32)\n",
    "            if age>=90:\n",
    "                age=89\n",
    "            if (age == 58) or (age == 59):\n",
    "                age = 60\n",
    "            \n",
    "            label = self.CLASS_MAP[gender]+self.CLASS_MAP[age//30] +self.CLASS_MAP[re.sub(rePattern,'',filename.split('.')[0]).strip()]\n",
    "        \n",
    "            if label not in self.CLASS_TO_ONEHOT_DICT:\n",
    "                one_hot_vector[label]=1\n",
    "                self.CLASS_TO_ONEHOT_DICT[label] = one_hot_vector\n",
    "                self.CLASS_TO_LABEL_DICT[label] = {'gender':gender, 'age':self.AGE_MAP[age//30], 'filename':re.sub(rePattern,'',filename.split('.')[0])}\n",
    "            else:\n",
    "                one_hot_vector = self.CLASS_TO_ONEHOT_DICT[label]\n",
    "            \n",
    "            return one_hot_vector, label\n",
    "        \n",
    "        self.train_y = []\n",
    "        self.train_X = []\n",
    "        \n",
    "        self.val_y = []\n",
    "        self.val_X = []\n",
    "        \n",
    "        \n",
    "        image_path_list = []\n",
    "        \n",
    "        for row in self.train_data_path_dataframe.iterrows():\n",
    "            image_directory_path = os.path.join(self.train_images_path, row[1]['path'])\n",
    "            image_file_name_list = os.listdir(image_directory_path)\n",
    "            for image_file_name in image_file_name_list:\n",
    "                if image_file_name[0]=='.': continue\n",
    "                image_path_list.append([os.path.join(image_directory_path, image_file_name), row[1]['gender'], row[1]['age']])\n",
    "        \n",
    "        \n",
    "        self.val_num_per_class = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0, 10:0, \n",
    "                                  11:0, 12:0, 13:0, 14:0, 15:0, 16:0, 17:0}\n",
    "        \n",
    "        self.class_rate = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0, 10:0, \n",
    "                                  11:0, 12:0, 13:0, 14:0, 15:0, 16:0, 17:0}\n",
    "        random.seed(seed)\n",
    "        random.shuffle(image_path_list)\n",
    "        \n",
    "        for i, [image_path, gender, age] in enumerate(image_path_list):\n",
    "            one_hot_vector, label = gen_onehot_label(gender, age, image_path.split('/')[-1])\n",
    "            if self.val_num_per_class[label] < n_val_per_class:\n",
    "                self.val_X.append(image_path)\n",
    "                self.val_y.append(label)\n",
    "                self.val_num_per_class[label] += 1\n",
    "            else:\n",
    "                self.train_X.append(image_path)\n",
    "                self.train_y.append(label)\n",
    "            self.class_rate[label] += 1\n",
    "               \n",
    "        if self.train:\n",
    "            del self.val_y\n",
    "            del self.val_X\n",
    "        else:\n",
    "            del self.train_y\n",
    "            del self.train_X\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.train_X)\n",
    "        else:\n",
    "            return len(self.val_X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X ,y = None, None\n",
    "        \n",
    "        if self.train:\n",
    "            X = self.transform(cv2.imread(self.train_X[idx]))\n",
    "            y = torch.tensor(self.CLASS_TO_ONEHOT_DICT[self.train_y[idx]],dtype=torch.float32)\n",
    "            \n",
    "        else:\n",
    "            X = self.transform(Image.open(self.val_X[idx]))\n",
    "            y = torch.tensor(self.CLASS_TO_ONEHOT_DICT[self.val_y[idx]],dtype=torch.float32)\n",
    "        return y, X\n",
    "    \n",
    "    \n",
    "    def onehot_tensor_to_class(self, onehot_tensor):\n",
    "        return torch.argmax(onehot_tensor).item()\n",
    "    \n",
    "    def onehot_tensor_to_gender_age_filename_dict(self, onehot_tensor):\n",
    "        return self.CLASS_TO_LABEL_DICT[self.onehot_list_to_class(onehot_tensor)]\n",
    "    \n",
    "    def gender_age_filename_to_onehot_list(self, gender, age, filename):\n",
    "        rePattern = re.compile('[0-9]')\n",
    "        one_hot_vector = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "        if age>=90:\n",
    "            age=89\n",
    "            \n",
    "        if (age == 58) or (age == 59):\n",
    "                age = 60\n",
    "                \n",
    "        label = self.CLASS_MAP[gender]+self.CLASS_MAP[age//30] +self.CLASS_MAP[re.sub(rePattern,'',filename.split('.')[0]).strip()]\n",
    "        if label in self.CLASS_TO_ONEHOT_DICT:\n",
    "            return self.CLASS_TO_ONEHOT_DICT[label]\n",
    "        else:\n",
    "            return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eecc0a89-568c-4225-9884-eb47ea91f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class specificClassDataModel(Dataset):\n",
    "    def __init__(self, dataModel, specificClass, max_data_num):\n",
    "        self.idxs = []\n",
    "        \n",
    "        self.dataModel = dataModel\n",
    "        \n",
    "        for i,(y, X) in tqdm(enumerate(dataModel),position=0,leave=True):\n",
    "            if int(dataModel.onehot_tensor_to_class(y)) == specificClass:\n",
    "                self.idxs.append(i)\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "    def __getitem__(self, idx):\n",
    "        y, X = None, None\n",
    "        \n",
    "        idx = self.idxs[idx]\n",
    "        y, X = self.dataModel[idx]\n",
    "            \n",
    "        return y, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ac24ffa-e4a5-4556-bb6f-e4e66ed1933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pretextMaskDataset(Dataset):\n",
    "    def __init__(self, data_path, transform, seed = 10):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.transform = transform\n",
    "    \n",
    "        self.train_data_path = os.path.join(data_path, 'train')\n",
    "        self.train_images_path = os.path.join(self.train_data_path, 'images')\n",
    "        self.train_data_csv_path = os.path.join(self.train_data_path, 'train.csv')\n",
    "        \n",
    "        self.train_data_path_dataframe = pd.read_csv(self.train_data_csv_path)\n",
    "        \n",
    "        self.y = []\n",
    "        self.X = []\n",
    "        \n",
    "        image_path_list = []\n",
    "        \n",
    "        self.position = {0:(0,0), 1:(0,1), 2:(1,0), 3:(1,1)}\n",
    "        \n",
    "        for row in self.train_data_path_dataframe.iterrows():\n",
    "            image_directory_path = os.path.join(self.train_images_path, row[1]['path'])\n",
    "            image_file_name_list = os.listdir(image_directory_path)\n",
    "            for image_file_name in image_file_name_list:\n",
    "                if image_file_name[0]=='.': continue\n",
    "                for rotation in range(4):\n",
    "                    image_path_list.append([os.path.join(image_directory_path, image_file_name), rotation])\n",
    "        \n",
    "\n",
    "        random.seed(seed)\n",
    "        random.shuffle(image_path_list)\n",
    "        \n",
    "        for i, [image_path, rotation] in enumerate(image_path_list):\n",
    "            self.X.append(image_path)\n",
    "            self.y.append(rotation)\n",
    "               \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X ,y = None, None\n",
    "        rand = random.randrange(0, len(self.X))\n",
    "        \n",
    "        \n",
    "        img = cv2.imread(self.X[idx])\n",
    "        center_y = img.shape[0]//2\n",
    "        center_x = img.shape[1]//2\n",
    "        position = self.position[self.y[idx]]\n",
    "        \n",
    "        start_x = center_x*position[0]\n",
    "        start_y = center_y*position[1]\n",
    "        end_x = center_x*(position[0]+1)\n",
    "        end_y = center_y*(position[1]+1)\n",
    "        \n",
    "        rand_img = cv2.imread(self.X[rand])\n",
    "        img[start_y:end_y, start_x:end_x,:] = rand_img[start_y:end_y, start_x:end_x,:]\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        rand_img = cv2.cvtColor(rand_img, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(img)\n",
    "        \n",
    "        \n",
    "        X = self.transform(img)\n",
    "        \n",
    "        one_hot = [0,0,0,0]\n",
    "        one_hot[self.y[idx]] = 1\n",
    "        y = torch.tensor(one_hot,dtype=torch.float)\n",
    "            \n",
    "        \n",
    "        return y, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9903f7b-7d7b-4eb6-bd6d-0c1db8f66bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class weighted_probability_cutmix_Dataset(Dataset):\n",
    "    def __init__(self, data_path, dataset, transform, class_num = 18):\n",
    "        self.X = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.dataset = maskDataset(data_path, transform = self.transform)\n",
    "        self.train_dataset_per_class_list = []\n",
    "        \n",
    "        \n",
    "        for i in tqdm(range(class_num)):\n",
    "            train_dataset = self.dataset\n",
    "            class_n_train_dataset = specificClassDataset(train_dataset,i,self.dataset.class_rate[i])\n",
    "            self.train_dataset_per_class_list.append(class_n_train_dataset)\n",
    "\n",
    "        cnt = 0\n",
    "        for idx in range(len(self.dataset)):\n",
    "            weights = sum(list(self.dataset.class_rate.values())) - np.array(list(self.dataset.class_rate.values()))\n",
    "            weights = list(weights)\n",
    "            mix_r_label = random.choices(range(0, 18), weights=weights)[0]\n",
    "            \n",
    "            mix_r_idx = random.randint(0,len(self.train_dataset_per_class_list[mix_r_label])-1)\n",
    "            self.X.append({'mix_r_label':mix_r_label, 'mix_r_idx':mix_r_idx})\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        X ,y = None, None\n",
    "        mix_r_label = self.X[idx]['mix_r_label']\n",
    "        mix_r_idx = self.X[idx]['mix_r_idx']\n",
    "        \n",
    "        \n",
    "        label, img = self.dataset[idx]\n",
    "        center_y = img.shape[0]//2\n",
    "        center_x = img.shape[1]//2\n",
    "        position = (0,0)\n",
    "        \n",
    "        start_x = center_x*position[0]\n",
    "        start_y = center_y*position[1]\n",
    "        end_x = center_x*(position[0]+1)\n",
    "        end_y = center_y*(position[1]+1)\n",
    "        \n",
    "        rand_label, rand_img = self.train_dataset_per_class_list[mix_r_label][mix_r_idx]\n",
    "        img[start_y:end_y, start_x:end_x,:] = rand_img[start_y:end_y, start_x:end_x,:]\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(img)\n",
    "        \n",
    "        X = self.transform(img)\n",
    "        y = 0.75*label + 0.25*rand_label\n",
    "            \n",
    "        \n",
    "        return y, X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef7b5d15-3428-4c7e-8932-ef290ce0cc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pytorchViTModel(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ViTModel = ViT('B_16_imagenet1k', pretrained=True)\n",
    "        self.linear = torch.nn.Linear(1000, 18, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.ViTModel(x)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b96ca4ec-e1d9-4b31-9727-54927e6634b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preTaskModel(Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lastLinear = torch.nn.Linear(18, 4, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return self.lastLinear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3463b460-a3cc-476a-94f8-bdcbe3820d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss_acc(model, loss_fn, dataLoader):\n",
    "    model.eval()\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    loss_sum = 0\n",
    "    with torch.no_grad():\n",
    "        for i,(y,X) in enumerate(dataLoader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(X)\n",
    "            loss = loss_fn(output, y)\n",
    "            loss_sum += loss.item()\n",
    "            target = torch.max(y,1)\n",
    "            y_pred = torch.max(output,1)\n",
    "            n_correct += (target.indices == y_pred.indices).sum()\n",
    "            n_total += y.size(0)\n",
    "    loss_avg = loss_sum/len(dataLoader)\n",
    "    acc = n_correct/n_total\n",
    "    model.train()\n",
    "    return loss_avg, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4575b81f-5d30-4313-b531-ab795fe1a744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss_acc_per_class(model, loss_fn, dataLoaderList):\n",
    "    for i, dataLoader in enumerate(dataLoaderList):\n",
    "        val_loss_avg, val_acc = model_loss_acc(model, loss_fn, dataLoader)\n",
    "        print('클래스 {}에 대한 loss = [{}], acc = [{}]'.format( i, val_loss_avg, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "757bb4a0-0292-405b-bb30-d5aec144b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "EPOCHS = 10\n",
    "BATCHS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c599dc98-96d3-4d7d-bcfb-61644c88a6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2745, 1: 1570, 2: 895, 3: 3660, 4: 3345, 5: 1285, 6: 549, 7: 314, 8: 179, 9: 732, 10: 669, 11: 257, 12: 549, 13: 314, 14: 179, 15: 732, 16: 669, 17: 257}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/18 [00:00<?, ?it/s]<ipython-input-5-06a108c83f58>:101: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(self.CLASS_TO_ONEHOT_DICT[self.val_y[idx]],dtype=torch.float32)\n",
      "1260it [00:06, 192.99it/s]\n",
      "1260it [00:06, 192.45it/s]06<01:51,  6.53s/it]\n",
      "1260it [00:06, 195.17it/s]13<01:44,  6.54s/it]\n",
      "1260it [00:06, 194.25it/s]19<01:37,  6.51s/it]\n",
      "1260it [00:06, 190.28it/s]26<01:31,  6.51s/it]\n",
      "1260it [00:06, 193.78it/s]32<01:25,  6.54s/it]\n",
      "1260it [00:06, 193.74it/s]39<01:18,  6.53s/it]\n",
      "1260it [00:06, 193.44it/s]45<01:11,  6.53s/it]\n",
      "1260it [00:06, 194.36it/s]52<01:05,  6.52s/it]\n",
      "1260it [00:06, 191.65it/s]58<00:58,  6.51s/it]\n",
      "1260it [00:06, 194.68it/s]:05<00:52,  6.53s/it]\n",
      "1260it [00:06, 193.77it/s]:11<00:45,  6.52s/it]\n",
      "1260it [00:06, 194.05it/s]:18<00:39,  6.51s/it]\n",
      "1260it [00:06, 194.19it/s]:24<00:32,  6.51s/it]\n",
      "1260it [00:06, 193.58it/s]:31<00:26,  6.50s/it]\n",
      "1260it [00:06, 193.08it/s]:37<00:19,  6.51s/it]\n",
      "1260it [00:06, 194.35it/s]:44<00:13,  6.51s/it]\n",
      "1260it [00:06, 194.00it/s]:50<00:06,  6.51s/it]\n",
      "100%|██████████| 18/18 [01:57<00:00,  6.52s/it]\n"
     ]
    }
   ],
   "source": [
    "#학습용 데이터셋\n",
    "mask_train_dataset = maskDataset(DATA_PATH,\n",
    "                                 transform = transforms.Compose([]),\n",
    "                                 n_val_per_class = 70,\n",
    "                                 train = True\n",
    "                          )\n",
    "print(mask_train_dataset.class_rate)\n",
    "\n",
    "\n",
    "#검증용 데이터셋 (전체 클래스)\n",
    "mask_val_dataset = maskDataset(DATA_PATH, \n",
    "                               transform = transforms.Compose([transforms.Resize((384, 384)), \n",
    "                                                           transforms.ToTensor(),\n",
    "                                                           transforms.Normalize(0.1, 0.5),]),\n",
    "                               n_val_per_class = 70,\n",
    "                               train = False\n",
    "                          )\n",
    "maskValDataLoader = DataLoader(mask_val_dataset, batch_size=1)\n",
    "\n",
    "#검증용 데이터셋(특정 클래스)\n",
    "val_dataloader_per_class_list = []\n",
    "for i in tqdm(range(18)):\n",
    "    val_dataset = mask_val_dataset\n",
    "    class_n_val_dataset = specificClassDataModel(val_dataset,i,mask_val_dataset.class_rate[i])\n",
    "    class_n_val_dataloader = DataLoader(class_n_val_dataset, batch_size=1)\n",
    "    val_dataloader_per_class_list.append(class_n_val_dataloader)\n",
    "\n",
    "\n",
    "pretext_task_dataset = pretextMaskDataset(DATA_PATH,\n",
    "                                          transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                                          transforms.Resize((384, 384)),\n",
    "                                                                          transforms.Normalize(0.1, 0.5),])\n",
    "                                         )\n",
    "pretextTaskDataLoader = DataLoader(pretext_task_dataset, batch_size=BATCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e74dd252-d9fe-4fdb-90ad-81788fe9285f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/18 [00:00<?, ?it/s]<ipython-input-5-06a108c83f58>:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(self.CLASS_TO_ONEHOT_DICT[self.train_y[idx]],dtype=torch.float32)\n",
      "17640it [00:46, 377.63it/s]\n",
      "40it [00:00, 398.67it/s]0:46<13:14, 46.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2745 2675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17640it [00:46, 375.40it/s]\n",
      "40it [00:00, 396.91it/s]1:33<12:28, 46.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1570 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17640it [00:46, 376.99it/s]\n",
      "41it [00:00, 400.65it/s]2:20<11:41, 46.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 895 825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17640it [00:46, 376.84it/s]\n",
      "40it [00:00, 397.96it/s]3:07<10:55, 46.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3660 3590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17640it [00:46, 376.62it/s]\n",
      "40it [00:00, 397.94it/s]3:54<10:08, 46.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 3345 3275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17640it [00:46, 377.09it/s]\n",
      "40it [00:00, 396.28it/s]4:40<09:21, 46.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 1285 1215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17640it [00:46, 376.71it/s]\n",
      "40it [00:00, 398.42it/s]5:27<08:34, 46.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 549 479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17640it [00:46, 376.55it/s]\n",
      "40it [00:00, 397.25it/s]6:14<07:48, 46.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 314 244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17640it [00:46, 377.06it/s]\n",
      "40it [00:00, 397.50it/s]7:01<07:01, 46.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 179 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17640it [00:46, 376.33it/s]\n",
      "40it [00:00, 397.68it/s]07:48<06:14, 46.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 732 662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17640it [00:46, 376.68it/s]\n",
      "40it [00:00, 397.90it/s]08:35<05:27, 46.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 669 599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17640it [00:46, 376.38it/s]\n",
      "41it [00:00, 401.32it/s]09:21<04:41, 46.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 257 187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17640it [00:46, 377.09it/s]\n",
      "40it [00:00, 398.29it/s]10:08<03:54, 46.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 549 479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17640it [00:46, 377.35it/s]\n",
      "41it [00:00, 401.60it/s]10:55<03:07, 46.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 314 244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17640it [00:46, 380.90it/s]\n",
      "41it [00:00, 402.08it/s]11:41<02:19, 46.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 179 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17640it [00:46, 378.53it/s]\n",
      "41it [00:00, 400.46it/s]12:28<01:33, 46.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 732 662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17640it [00:46, 379.87it/s]\n",
      "40it [00:00, 398.81it/s]13:14<00:46, 46.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 669 599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17640it [00:46, 380.60it/s]\n",
      "100%|██████████| 18/18 [14:01<00:00, 46.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 257 187\n"
     ]
    }
   ],
   "source": [
    "#학습용 데이터셋(특정 클래스)\n",
    "train_dataloader_per_class_list = []\n",
    "for i in tqdm(range(18)):\n",
    "    train_dataset = mask_train_dataset\n",
    "    class_n_train_dataset = specificClassDataModel(train_dataset,i,mask_train_dataset.class_rate[i])\n",
    "    print(i, mask_train_dataset.class_rate[i], len(class_n_train_dataset))\n",
    "    class_n_train_dataloader = DataLoader(class_n_train_dataset, batch_size=1)\n",
    "    train_dataloader_per_class_list.append(class_n_train_dataloader)\n",
    "    \n",
    "weighted_cutmix_Dataset = weighted_probability_cutmix_Dataset(mask_train_dataset,\n",
    "                                                                            train_dataloader_per_class_list,\n",
    "                                                                           transform = transforms.Compose([\n",
    "                                                                                 transforms.Resize((384, 384)), \n",
    "                                                                                 transforms.ToTensor(),\n",
    "                                                                                 transforms.Normalize(0.1, 0.5),\n",
    "                                                                             ]),)\n",
    "maskTrainDataLoader = DataLoader(weighted_cutmix_Dataset, batch_size=BATCHS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb7cb9a5-e65a-42dd-aa5e-fadb17e3a0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights.\n"
     ]
    }
   ],
   "source": [
    "model = pytorchViTModel()\n",
    "model.train()\n",
    "params = [param for param in model.parameters() if param.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=lr)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1361dbbf-5a38-4afa-ab0d-6f54c4fc52cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9450/9450 [58:03<00:00,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss = [0.1234074942875093], train_acc = [0.9130820105820106]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preModel = preTaskModel(model).to(device)\n",
    "for e in range(1):\n",
    "    train_loss_sum = 0\n",
    "    n_correct, n_total = 0, 0\n",
    "    for y,X in tqdm(pretextTaskDataLoader,position =0, leave=True):\n",
    "        y = y.to(device)\n",
    "        X = X.to(device)\n",
    "        output = preModel(X)\n",
    "        loss = loss_fn(output, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_sum += loss.item()\n",
    "        target = torch.max(y,1)\n",
    "        y_pred = torch.max(output,1)\n",
    "        n_correct += (target.indices == y_pred.indices).sum().item()\n",
    "        n_total += y.size(0)\n",
    "        \n",
    "        del X\n",
    "        del y\n",
    "        del output\n",
    "        \n",
    "    train_loss_avg = train_loss_sum/len(maskTrainDataLoader)\n",
    "    train_acc = n_correct/n_total\n",
    "    print('train_loss = [{}], train_acc = [{}]'.format( train_loss_avg, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c15a804c-f600-426b-9294-289bf6553db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss = [0.1234074942875093], train_acc = [0.9130820105820106]\n"
     ]
    }
   ],
   "source": [
    "print('train_loss = [{}], train_acc = [{}]'.format( train_loss_avg, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d013c71c-fbd0-4ff1-a62a-d24eafa9ff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dc94852-d0e5-4e30-9d93-06699b1cbf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(model, dataLoader):\n",
    "    model.eval()\n",
    "    tp, fp, tn, fn = 0, 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for y, X in dataLoader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(X)\n",
    "            target = y\n",
    "            y_pred = output\n",
    "            tp += (target*y_pred).sum().item()\n",
    "            tn += ((1-target)*(1-y_pred)).sum().item()\n",
    "            fp += ((1-target)*y_pred).sum().item()\n",
    "            fn += (target*(1-y_pred)).sum().item()\n",
    "    \n",
    "    precision = tp / (tp+fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    epsilon = 1e-7\n",
    "    f1 = (2*precision*recall)/(recall + precision + epsilon)\n",
    "    \n",
    "    model.train()\n",
    "    return f1\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fd978be-d325-41a9-860f-dbb1fd7ffa6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgeup\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "2021-09-02 02:14:27.162795: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2021-09-02 02:14:27.163410: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\n",
      "CondaEnvException: Unable to determine environment\n",
      "\n",
      "Please re-run this command with one of the following options:\n",
      "\n",
      "* Provide an environment name via --name or -n\n",
      "* Re-run this command inside an activated conda environment.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.0<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">lucky-sound-54</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/geup/mask_image_classification\" target=\"_blank\">https://wandb.ai/geup/mask_image_classification</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/geup/mask_image_classification/runs/filshy23\" target=\"_blank\">https://wandb.ai/geup/mask_image_classification/runs/filshy23</a><br/>\n",
       "                Run data is saved locally in <code>/opt/ml/code/wandb/run-20210902_021425-filshy23</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:filshy23) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9176<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/opt/ml/code/wandb/run-20210902_021425-filshy23/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/opt/ml/code/wandb/run-20210902_021425-filshy23/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">lucky-sound-54</strong>: <a href=\"https://wandb.ai/geup/mask_image_classification/runs/filshy23\" target=\"_blank\">https://wandb.ai/geup/mask_image_classification/runs/filshy23</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:filshy23). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "2021-09-02 02:14:36.921766: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2021-09-02 02:14:36.922388: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\n",
      "CondaEnvException: Unable to determine environment\n",
      "\n",
      "Please re-run this command with one of the following options:\n",
      "\n",
      "* Provide an environment name via --name or -n\n",
      "* Re-run this command inside an activated conda environment.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.0<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">clear-forest-55</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/geup/mask_image_classification\" target=\"_blank\">https://wandb.ai/geup/mask_image_classification</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/geup/mask_image_classification/runs/2hv2tfw5\" target=\"_blank\">https://wandb.ai/geup/mask_image_classification/runs/2hv2tfw5</a><br/>\n",
       "                Run data is saved locally in <code>/opt/ml/code/wandb/run-20210902_021431-2hv2tfw5</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(2hv2tfw5)</h1><iframe src=\"https://wandb.ai/geup/mask_image_classification/runs/2hv2tfw5\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fa57408a7c0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"mask_image_classification\", entity=\"geup\")\n",
    "config = {\"epochs\":EPOCHS, \"batch_size\":BATCHS, \"learning_rate\":lr}\n",
    "wandb.init(project=\"mask_image_classification\", config=config)\n",
    "wandb.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d124591-31df-4703-9f7c-299473da5980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2205 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 74% |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-06a108c83f58>:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(self.CLASS_TO_ONEHOT_DICT[self.train_y[idx]],dtype=torch.float32)\n",
      "100%|██████████| 2205/2205 [16:06<00:00,  2.28it/s]\n",
      "<ipython-input-5-06a108c83f58>:101: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(self.CLASS_TO_ONEHOT_DICT[self.val_y[idx]],dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss = [0.0009538077902222525], train_acc = [0.9901927437641723] val_loss = [0.005880929050796286], val_acc = [0.9547619223594666], val_f1 = [0.8615707383578879]\n",
      "클래스 0에 대한 loss = [0.001258272973219781], acc = [1.0]\n",
      "클래스 1에 대한 loss = [0.003411058652792625], acc = [0.9714285731315613]\n",
      "클래스 2에 대한 loss = [0.004613734048169655], acc = [0.9428571462631226]\n",
      "클래스 3에 대한 loss = [0.0009868984265527356], acc = [1.0]\n",
      "클래스 4에 대한 loss = [0.004428680356535811], acc = [0.9714285731315613]\n",
      "클래스 5에 대한 loss = [0.00352922919638721], acc = [0.9714285731315613]\n",
      "클래스 6에 대한 loss = [0.003593014628651352], acc = [0.985714316368103]\n",
      "클래스 7에 대한 loss = [0.005541979949235351], acc = [0.9571428894996643]\n",
      "클래스 8에 대한 loss = [0.014116256270790472], acc = [0.8571428656578064]\n",
      "클래스 9에 대한 loss = [0.004922349193865167], acc = [0.9571428894996643]\n",
      "클래스 10에 대한 loss = [0.004625479198979779], acc = [0.9714285731315613]\n",
      "클래스 11에 대한 loss = [0.009345093292150913], acc = [0.9142857193946838]\n",
      "클래스 12에 대한 loss = [0.004556865955237299], acc = [0.9714285731315613]\n",
      "클래스 13에 대한 loss = [0.009277149928467614], acc = [0.9714285731315613]\n",
      "클래스 14에 대한 loss = [0.017422100539053124], acc = [0.8285714387893677]\n",
      "클래스 15에 대한 loss = [0.0035573554372863975], acc = [0.9714285731315613]\n",
      "클래스 16에 대한 loss = [0.004357964096665715], acc = [0.9571428894996643]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2205 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 17에 대한 loss = [0.006313240770292136], acc = [0.985714316368103]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2205/2205 [16:09<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss = [0.0011726550183760777], train_acc = [0.9861678004535147] val_loss = [0.004540028067545098], val_acc = [0.9698413014411926], val_f1 = [0.8890753661233047]\n",
      "클래스 0에 대한 loss = [0.0019351188151111793], acc = [0.985714316368103]\n",
      "클래스 1에 대한 loss = [0.0015556331366367107], acc = [1.0]\n",
      "클래스 2에 대한 loss = [0.0017349457184796587], acc = [1.0]\n",
      "클래스 3에 대한 loss = [0.0008060072977968957], acc = [1.0]\n",
      "클래스 4에 대한 loss = [0.004432206960180858], acc = [0.9714285731315613]\n",
      "클래스 5에 대한 loss = [0.002388965369547285], acc = [0.985714316368103]\n",
      "클래스 6에 대한 loss = [0.003035031770819582], acc = [0.985714316368103]\n",
      "클래스 7에 대한 loss = [0.006906864688166284], acc = [0.9571428894996643]\n",
      "클래스 8에 대한 loss = [0.005682170605307744], acc = [0.9571428894996643]\n",
      "클래스 9에 대한 loss = [0.005790118319470951], acc = [0.9428571462631226]\n",
      "클래스 10에 대한 loss = [0.005507567523454782], acc = [0.9571428894996643]\n",
      "클래스 11에 대한 loss = [0.006986018928104645], acc = [0.9714285731315613]\n",
      "클래스 12에 대한 loss = [0.004830992006723786], acc = [0.9571428894996643]\n",
      "클래스 13에 대한 loss = [0.006311095880144941], acc = [0.9571428894996643]\n",
      "클래스 14에 대한 loss = [0.008643493904050307], acc = [0.9285714626312256]\n",
      "클래스 15에 대한 loss = [0.004189788378633759], acc = [0.9714285731315613]\n",
      "클래스 16에 대한 loss = [0.006156481166596807], acc = [0.9428571462631226]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2205 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 17에 대한 loss = [0.004828004746585585], acc = [0.985714316368103]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2205/2205 [16:06<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss = [0.0009968140870522783], train_acc = [0.9884353741496599] val_loss = [0.006450869060559882], val_acc = [0.9396825432777405], val_f1 = [0.8754101939999677]\n",
      "클래스 0에 대한 loss = [0.0026728378275168193], acc = [0.985714316368103]\n",
      "클래스 1에 대한 loss = [0.0051101172004044725], acc = [0.9428571462631226]\n",
      "클래스 2에 대한 loss = [0.011538635458938578], acc = [0.8857142925262451]\n",
      "클래스 3에 대한 loss = [0.0006849927291373855], acc = [1.0]\n",
      "클래스 4에 대한 loss = [0.004974467891224776], acc = [0.9428571462631226]\n",
      "클래스 5에 대한 loss = [0.0049908329639168055], acc = [0.9428571462631226]\n",
      "클래스 6에 대한 loss = [0.004555361341460541], acc = [0.9571428894996643]\n",
      "클래스 7에 대한 loss = [0.004588314308784902], acc = [0.9714285731315613]\n",
      "클래스 8에 대한 loss = [0.008215465347062231], acc = [0.9285714626312256]\n",
      "클래스 9에 대한 loss = [0.0037264877615858236], acc = [0.9714285731315613]\n",
      "클래스 10에 대한 loss = [0.008278354461487782], acc = [0.9285714626312256]\n",
      "클래스 11에 대한 loss = [0.010845166150413985], acc = [0.8857142925262451]\n",
      "클래스 12에 대한 loss = [0.004662031092032391], acc = [0.9714285731315613]\n",
      "클래스 13에 대한 loss = [0.006197791984283997], acc = [0.9714285731315613]\n",
      "클래스 14에 대한 loss = [0.016234906877902435], acc = [0.8285714387893677]\n",
      "클래스 15에 대한 loss = [0.004333694710554222], acc = [0.9428571462631226]\n",
      "클래스 16에 대한 loss = [0.006704904713946494], acc = [0.9142857193946838]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2205 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 17에 대한 loss = [0.007801280269424231], acc = [0.9428571462631226]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2205/2205 [16:07<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss = [0.0008837998913532538], train_acc = [0.990249433106576] val_loss = [0.01131397735022354], val_acc = [0.8904762268066406], val_f1 = [0.7646924607663924]\n",
      "클래스 0에 대한 loss = [0.0015832821443577164], acc = [1.0]\n",
      "클래스 1에 대한 loss = [0.0043044274310107405], acc = [0.9714285731315613]\n",
      "클래스 2에 대한 loss = [0.0018945383643897782], acc = [1.0]\n",
      "클래스 3에 대한 loss = [0.002935767546919773], acc = [0.9714285731315613]\n",
      "클래스 4에 대한 loss = [0.003748821640017143], acc = [0.9714285731315613]\n",
      "클래스 5에 대한 loss = [0.007327061019064526], acc = [0.9428571462631226]\n",
      "클래스 6에 대한 loss = [0.0054422827221320145], acc = [0.9571428894996643]\n",
      "클래스 7에 대한 loss = [0.020398173271470504], acc = [0.7714285850524902]\n",
      "클래스 8에 대한 loss = [0.02136186930916405], acc = [0.8142856955528259]\n",
      "클래스 9에 대한 loss = [0.01049641770992561], acc = [0.8714285492897034]\n",
      "클래스 10에 대한 loss = [0.02435301940921428], acc = [0.6857143044471741]\n",
      "클래스 11에 대한 loss = [0.017673336561503154], acc = [0.7571428418159485]\n",
      "클래스 12에 대한 loss = [0.005134836465627554], acc = [0.9571428894996643]\n",
      "클래스 13에 대한 loss = [0.014231923149366464], acc = [0.9142857193946838]\n",
      "클래스 14에 대한 loss = [0.008275542724212366], acc = [0.9714285731315613]\n",
      "클래스 15에 대한 loss = [0.00462747396335804], acc = [0.9571428894996643]\n",
      "클래스 16에 대한 loss = [0.0170480076489704], acc = [0.9285714626312256]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2205 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 17에 대한 loss = [0.0328148112233196], acc = [0.5857142806053162]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2205/2205 [16:14<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss = [0.0010349848804117156], train_acc = [0.9877551020408163] val_loss = [0.0063792487453380115], val_acc = [0.9515873193740845], val_f1 = [0.83606002136821]\n",
      "클래스 0에 대한 loss = [0.001439154486537778], acc = [1.0]\n",
      "클래스 1에 대한 loss = [0.005497440429358643], acc = [0.9428571462631226]\n",
      "클래스 2에 대한 loss = [0.003297561837825924], acc = [1.0]\n",
      "클래스 3에 대한 loss = [0.001504159349756914], acc = [1.0]\n",
      "클래스 4에 대한 loss = [0.0065740734652665975], acc = [0.9571428894996643]\n",
      "클래스 5에 대한 loss = [0.0030167136601189017], acc = [0.985714316368103]\n",
      "클래스 6에 대한 loss = [0.0051810473868889465], acc = [0.9714285731315613]\n",
      "클래스 7에 대한 loss = [0.01050802866853441], acc = [0.9142857193946838]\n",
      "클래스 8에 대한 loss = [0.0046807290915499575], acc = [0.9714285731315613]\n",
      "클래스 9에 대한 loss = [0.006401527910825929], acc = [0.9428571462631226]\n",
      "클래스 10에 대한 loss = [0.01037770354908259], acc = [0.9142857193946838]\n",
      "클래스 11에 대한 loss = [0.013409689981199336], acc = [0.8571428656578064]\n",
      "클래스 12에 대한 loss = [0.0048266436626103575], acc = [0.9571428894996643]\n",
      "클래스 13에 대한 loss = [0.004384503399446008], acc = [0.985714316368103]\n",
      "클래스 14에 대한 loss = [0.0069775098822512], acc = [0.985714316368103]\n",
      "클래스 15에 대한 loss = [0.006427215580749492], acc = [0.9571428894996643]\n",
      "클래스 16에 대한 loss = [0.006564923739434952], acc = [0.9571428894996643]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2205 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 17에 대한 loss = [0.013757851334646277], acc = [0.8285714387893677]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2205/2205 [16:04<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss = [0.0010006957490904184], train_acc = [0.988265306122449] val_loss = [0.006959598308247454], val_acc = [0.9460317492485046], val_f1 = [0.8315894629943965]\n",
      "클래스 0에 대한 loss = [0.005168473485744991], acc = [0.9571428894996643]\n",
      "클래스 1에 대한 loss = [0.007434866033664938], acc = [0.9142857193946838]\n",
      "클래스 2에 대한 loss = [0.006998010922896875], acc = [0.9142857193946838]\n",
      "클래스 3에 대한 loss = [0.001859053414331616], acc = [1.0]\n",
      "클래스 4에 대한 loss = [0.0034341870526986896], acc = [0.985714316368103]\n",
      "클래스 5에 대한 loss = [0.003135839393196095], acc = [0.985714316368103]\n",
      "클래스 6에 대한 loss = [0.003257083011807741], acc = [0.985714316368103]\n",
      "클래스 7에 대한 loss = [0.00727164157786839], acc = [0.9428571462631226]\n",
      "클래스 8에 대한 loss = [0.012337910106206046], acc = [0.9285714626312256]\n",
      "클래스 9에 대한 loss = [0.005250786618645569], acc = [0.9571428894996643]\n",
      "클래스 10에 대한 loss = [0.006568291926149479], acc = [0.9571428894996643]\n",
      "클래스 11에 대한 loss = [0.01437808271148242], acc = [0.8999999761581421]\n",
      "클래스 12에 대한 loss = [0.007876187627594585], acc = [0.9285714626312256]\n",
      "클래스 13에 대한 loss = [0.008324652548513509], acc = [0.9714285731315613]\n",
      "클래스 14에 대한 loss = [0.01758080783349994], acc = [0.7714285850524902]\n",
      "클래스 15에 대한 loss = [0.0021710829001676756], acc = [1.0]\n",
      "클래스 16에 대한 loss = [0.004092570297611279], acc = [0.9571428894996643]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2205 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 17에 대한 loss = [0.008133242086374334], acc = [0.9714285731315613]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2205/2205 [16:10<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss = [0.000965428048928726], train_acc = [0.9891156462585035] val_loss = [0.00668839568080575], val_acc = [0.9492063522338867], val_f1 = [0.8556064200126967]\n",
      "클래스 0에 대한 loss = [0.0012763060192810371], acc = [1.0]\n",
      "클래스 1에 대한 loss = [0.0034241078893371326], acc = [0.9714285731315613]\n",
      "클래스 2에 대한 loss = [0.0009048678150325681], acc = [1.0]\n",
      "클래스 3에 대한 loss = [0.0007034431644569849], acc = [1.0]\n",
      "클래스 4에 대한 loss = [0.004064952115835955], acc = [0.9714285731315613]\n",
      "클래스 5에 대한 loss = [0.0031214646677004305], acc = [0.985714316368103]\n",
      "클래스 6에 대한 loss = [0.002781335275877999], acc = [0.985714316368103]\n",
      "클래스 7에 대한 loss = [0.017765251188705276], acc = [0.8285714387893677]\n",
      "클래스 8에 대한 loss = [0.012357728458092814], acc = [0.8999999761581421]\n",
      "클래스 9에 대한 loss = [0.005263909843883344], acc = [0.9571428894996643]\n",
      "클래스 10에 대한 loss = [0.008414703822096012], acc = [0.9285714626312256]\n",
      "클래스 11에 대한 loss = [0.012128989346404393], acc = [0.8999999761581421]\n",
      "클래스 12에 대한 loss = [0.00983101962483488], acc = [0.8999999761581421]\n",
      "클래스 13에 대한 loss = [0.008670226225513033], acc = [0.9428571462631226]\n",
      "클래스 14에 대한 loss = [0.006088309899704265], acc = [0.985714316368103]\n",
      "클래스 15에 대한 loss = [0.0025275087096295985], acc = [0.985714316368103]\n",
      "클래스 16에 대한 loss = [0.009054870421615695], acc = [0.9285714626312256]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2205 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 17에 대한 loss = [0.01201212776650209], acc = [0.9142857193946838]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2205/2205 [16:05<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss = [0.0008447787333315485], train_acc = [0.9901360544217687] val_loss = [0.00626162231939296], val_acc = [0.9484127163887024], val_f1 = [0.8600720442467025]\n",
      "클래스 0에 대한 loss = [0.0017033152982289072], acc = [0.985714316368103]\n",
      "클래스 1에 대한 loss = [0.002695620090006352], acc = [0.9714285731315613]\n",
      "클래스 2에 대한 loss = [0.0020416819131371866], acc = [1.0]\n",
      "클래스 3에 대한 loss = [0.0007763656605675351], acc = [1.0]\n",
      "클래스 4에 대한 loss = [0.007595811470985479], acc = [0.9285714626312256]\n",
      "클래스 5에 대한 loss = [0.0014984582381397818], acc = [1.0]\n",
      "클래스 6에 대한 loss = [0.00397198944280847], acc = [0.9714285731315613]\n",
      "클래스 7에 대한 loss = [0.007935126860788191], acc = [0.9428571462631226]\n",
      "클래스 8에 대한 loss = [0.009901315052827288], acc = [0.9285714626312256]\n",
      "클래스 9에 대한 loss = [0.00526339406117456], acc = [0.9285714626312256]\n",
      "클래스 10에 대한 loss = [0.00797377023845911], acc = [0.9285714626312256]\n",
      "클래스 11에 대한 loss = [0.008139674954665159], acc = [0.9428571462631226]\n",
      "클래스 12에 대한 loss = [0.006160347309923963], acc = [0.9571428894996643]\n",
      "클래스 13에 대한 loss = [0.009715154895820888], acc = [0.9428571462631226]\n",
      "클래스 14에 대한 loss = [0.01071337455900253], acc = [0.8999999761581421]\n",
      "클래스 15에 대한 loss = [0.004094200372700081], acc = [0.9714285731315613]\n",
      "클래스 16에 대한 loss = [0.011146566914976574], acc = [0.8714285492897034]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2205 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 17에 대한 loss = [0.011383034414861219], acc = [0.8999999761581421]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2205/2205 [16:09<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss = [0.000686339819682403], train_acc = [0.9930272108843538] val_loss = [0.006018825030274918], val_acc = [0.9523809552192688], val_f1 = [0.8650685299512875]\n",
      "클래스 0에 대한 loss = [0.001553065086773131], acc = [1.0]\n",
      "클래스 1에 대한 loss = [0.004103401703417019], acc = [0.9571428894996643]\n",
      "클래스 2에 대한 loss = [0.00805398256634362], acc = [0.8857142925262451]\n",
      "클래스 3에 대한 loss = [0.00447276705625492], acc = [0.9428571462631226]\n",
      "클래스 4에 대한 loss = [0.003280547383889955], acc = [1.0]\n",
      "클래스 5에 대한 loss = [0.0022375423650893415], acc = [1.0]\n",
      "클래스 6에 대한 loss = [0.0033947591893333344], acc = [0.985714316368103]\n",
      "클래스 7에 대한 loss = [0.006232112064546007], acc = [0.9571428894996643]\n",
      "클래스 8에 대한 loss = [0.011928196485262431], acc = [0.8999999761581421]\n",
      "클래스 9에 대한 loss = [0.0048900034335279735], acc = [0.9571428894996643]\n",
      "클래스 10에 대한 loss = [0.0041926841027036844], acc = [1.0]\n",
      "클래스 11에 대한 loss = [0.006677079075598158], acc = [0.9428571462631226]\n",
      "클래스 12에 대한 loss = [0.004003080864068969], acc = [0.985714316368103]\n",
      "클래스 13에 대한 loss = [0.009880531159329362], acc = [0.9571428894996643]\n",
      "클래스 14에 대한 loss = [0.011843197514619014], acc = [0.8857142925262451]\n",
      "클래스 15에 대한 loss = [0.007183138904500603], acc = [0.9285714626312256]\n",
      "클래스 16에 대한 loss = [0.003966421184824347], acc = [0.9571428894996643]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2205 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 17에 대한 loss = [0.010446340404866663], acc = [0.8999999761581421]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2205/2205 [16:09<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss = [0.0008994349047757818], train_acc = [0.9896258503401361] val_loss = [0.005599394695800295], val_acc = [0.9563491940498352], val_f1 = [0.8721784080886156]\n",
      "클래스 0에 대한 loss = [0.0009416043875847078], acc = [1.0]\n",
      "클래스 1에 대한 loss = [0.0036679276087982415], acc = [0.9571428894996643]\n",
      "클래스 2에 대한 loss = [0.004866769578023065], acc = [0.9428571462631226]\n",
      "클래스 3에 대한 loss = [0.0019200563387130388], acc = [0.985714316368103]\n",
      "클래스 4에 대한 loss = [0.003064798359680156], acc = [0.985714316368103]\n",
      "클래스 5에 대한 loss = [0.0026983001328127493], acc = [0.985714316368103]\n",
      "클래스 6에 대한 loss = [0.0040788665937725455], acc = [0.985714316368103]\n",
      "클래스 7에 대한 loss = [0.004728732099257675], acc = [0.985714316368103]\n",
      "클래스 8에 대한 loss = [0.013212710688198319], acc = [0.8857142925262451]\n",
      "클래스 9에 대한 loss = [0.004240412019342849], acc = [0.9714285731315613]\n",
      "클래스 10에 대한 loss = [0.004379057320017767], acc = [0.985714316368103]\n",
      "클래스 11에 대한 loss = [0.006320445543055289], acc = [0.9714285731315613]\n",
      "클래스 12에 대한 loss = [0.0036059391451999543], acc = [0.985714316368103]\n",
      "클래스 13에 대한 loss = [0.009447135248254718], acc = [0.9285714626312256]\n",
      "클래스 14에 대한 loss = [0.015659447156940585], acc = [0.8428571224212646]\n",
      "클래스 15에 대한 loss = [0.005085125844509873], acc = [0.9428571462631226]\n",
      "클래스 16에 대한 loss = [0.006322817481694594], acc = [0.9285714626312256]\n",
      "클래스 17에 대한 loss = [0.006548958978549178], acc = [0.9428571462631226]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "GPUtil.showUtilization()\n",
    "\n",
    "model.train()\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    train_loss_sum = 0\n",
    "    n_correct, n_total = 0, 0\n",
    "    val_loss_sum = 0\n",
    "    val_n_correct, val_n_total = 0,0\n",
    "    for y,X in tqdm(maskTrainDataLoader,position =0, leave=True):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        output = model(X)\n",
    "        loss = loss_fn(output, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_sum += loss.item()\n",
    "        target = torch.max(y,1)\n",
    "        y_pred = torch.max(output,1)\n",
    "        n_correct += (target.indices == y_pred.indices).sum().item()\n",
    "        n_total += y.size(0)\n",
    "        \n",
    "        del X\n",
    "        del y\n",
    "        del output\n",
    "        del loss\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    train_loss_avg = train_loss_sum/len(maskTrainDataLoader)\n",
    "    train_acc = n_correct/n_total\n",
    "    val_loss_avg, val_acc = model_loss_acc(model, loss_fn, maskValDataLoader)\n",
    "    f1 = f1_score(model, maskValDataLoader)\n",
    "    print('train_loss = [{}], train_acc = [{}] val_loss = [{}], val_acc = [{}], val_f1 = [{}]'.format( train_loss_avg, train_acc, val_loss_avg, val_acc, f1))\n",
    "    model_loss_acc_per_class(model, loss_fn, val_dataloader_per_class_list)\n",
    "    wandb.log({\"train_loss_avg\": train_loss_avg, \"train_acc\":train_acc, \"val_loss_avg\":val_loss_avg,\"val_acc\":val_acc, \"val_f1_score\":f1}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98356fc5-0f4f-4fac-99ba-04847354e477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights.\n"
     ]
    }
   ],
   "source": [
    "torch.save({'model':maskModel(), 'state_dict':model.state_dict()}, './vit_pretext_task_postion_cutmix.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "faa821df-99e2-4f13-8af0-cbb50534259d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './vit_pretext_task_position.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-88760a76c43e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./vit_pretext_task_position.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmyModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmyModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './vit_pretext_task_position.pt'"
     ]
    }
   ],
   "source": [
    "#checkpoint = torch.load('./vit_pretext_task_position.pt')\n",
    "#myModel = checkpoint['model']\n",
    "#myModel.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b8fd1be-19d6-4e34-8c3c-5031be14733a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 74% |\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e51b82-ecee-4d0b-acae-c76135c9af9c",
   "metadata": {},
   "source": [
    "## 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0f9d1f5-11d5-4aab-ae3c-804182fadb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68065cd1-ffde-4183-9f64-a05a482b31e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12600/12600 [05:14<00:00, 40.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터셋 폴더 경로를 지정해주세요.\n",
    "test_dir = '/opt/ml/input/data/eval'\n",
    "\n",
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((384,384)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(0.1, 0.5),\n",
    "])\n",
    "dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "all_softpredictions = []\n",
    "\n",
    "for images in tqdm(loader):\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        output = model(images)\n",
    "        pred = output.argmax(dim=-1)\n",
    "        soft_pred = torch.nn.Softmax(dim=1)(output)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "        all_softpredictions.extend(soft_pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "\n",
    "submission['ans'] = all_softpredictions\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'soft_submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88399e84-7861-49b9-ac0a-6ec7433926ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
